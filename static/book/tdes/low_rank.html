
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Low rank approximation of BEM matrices with adaptive cross approximation (ACA). &#8212; Integral equation tutorials</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/patch.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://tbenthompson.com/book/tdes/low_rank.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. GPU-accelerated hierarchical matrices for triangular dislocation elements." href="hmatrix.html" />
    <link rel="prev" title="3. Minimizing memory usage: a matrix-free iterative solver" href="free_matvec.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-114592151-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Integral equation tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Integral equation tutorials
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  The TDE sequence: triangular dislocation elements
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="sa_tdes.html">
   1. Using TDEs to build a fault model with topography.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sa_geometry.html">
   2. A fault and topography mesh of the South America subduction zone.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="free_matvec.html">
   3. Minimizing memory usage: a matrix-free iterative solver
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Low rank approximation of BEM matrices with adaptive cross approximation (ACA).
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hmatrix.html">
   5. GPU-accelerated hierarchical matrices for triangular dislocation elements.
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  The QBX sequence: quadrature by expansion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part1_nearfield.html">
   1. Computing boundary integrals with quadrature by expansion (QBX).
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part2_screw_dislocation.html">
   2. QBX examples for the Laplace equation: fun with screw dislocations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part3_surface_intersection.html">
   3. Slip on an infinite strike slip fault in an elastic half space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part4_topography.html">
   4. Topography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part5_viscoelastic.html">
   5. Viscoelasticity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part6_qd.html">
   6. Quasidynamic earthquake cycles: antiplane
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part7_qd_two_faults.html">
   7. Quasidynamic earthquake cycles on two faults
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1qbx/part8_thrust_qd.html">
   8. [DRAFT] Quasidynamic thrust fault earthquake cycles (plane strain)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  The volumetric sequence: beyond boundaries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../volumetric/part1_solve_circle.html">
   1. [DRAFT] Solving a Dirichlet problem on a circle.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../volumetric/part2_mms_circle.html">
   2. [DRAFT] Method of manufactured solutions (MMS) for the Poisson equation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Back matter
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../thanks.html">
   Thanks!
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/tdes/low_rank.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#near-field-vs-far-field">
   4.1. Near-field vs far-field
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approximation-with-the-singular-value-decomposition-svd">
   4.2. Approximation with the singular value decomposition (SVD)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaptive-cross-approximation-aca">
   4.3. Adaptive cross approximation (ACA)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aca-and-aca-the-details">
     4.3.1. ACA and ACA+, the details
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-aca">
     4.3.2. Implementing ACA+
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#aca-is-accurate-and-efficient">
   4.4. ACA+ is accurate and efficient
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svd-recompression">
   4.5. SVD Recompression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Low rank approximation of BEM matrices with adaptive cross approximation (ACA).</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#near-field-vs-far-field">
   4.1. Near-field vs far-field
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approximation-with-the-singular-value-decomposition-svd">
   4.2. Approximation with the singular value decomposition (SVD)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaptive-cross-approximation-aca">
   4.3. Adaptive cross approximation (ACA)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aca-and-aca-the-details">
     4.3.1. ACA and ACA+, the details
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-aca">
     4.3.2. Implementing ACA+
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#aca-is-accurate-and-efficient">
   4.4. ACA+ is accurate and efficient
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svd-recompression">
   4.5. SVD Recompression
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="low-rank-approximation-of-bem-matrices-with-adaptive-cross-approximation-aca">
<h1><span class="section-number">4. </span>Low rank approximation of BEM matrices with adaptive cross approximation (ACA).<a class="headerlink" href="#low-rank-approximation-of-bem-matrices-with-adaptive-cross-approximation-aca" title="Permalink to this headline">¶</a></h1>
<p>In the last section, I demonstrated how to avoid constructing the full dense BEM matrix by regenerating matrix entries whenever they are needed. This can be helpful for reducing memory costs and, in some situations, actually results in a faster solver too. But, it still suffers from the fundamental problem of having to work with a dense matrix where each matrix-vector product is an <span class="math notranslate nohighlight">\(O(n^2)\)</span> operation. Ultimately, for boundary integral methods to be a useful technology, we need to avoid working with dense matrices entirely.</p>
<p>So, here, I’ll be demonstrating how the off-diagonal blocks of a BEM matrix can be dramatically compressed via a low-rank approximation. The result will be <span class="math notranslate nohighlight">\(O(n\log n)\)</span> or <span class="math notranslate nohighlight">\(O(n)\)</span> solution methods that can scale up to millions of elements. Hierarchical matrices (aka H-matrices,<span id="id1">[<a class="reference internal" href="../references.html#id85">Bebendorf, 2008</a>]</span>), tree-codes <span id="id2">[<a class="reference internal" href="../references.html#id62">Barnes and Hut, 1986</a>]</span>, fast multipole methods (FMM, <span id="id3">[<a class="reference internal" href="../references.html#id77">Beatson and Greengard, 1997</a>]</span>) and several other techniques all make use of this basic concept. I’d argue these linear or log-linear methods are the largest distinguishing factor between “basic” BEM approaches from “advanced” or “modern” BEM approaches. So, let’s dive into a tour of low rank matrices! The tools we build here will form the core of the hierarchical matrix (H-matrix) implementation in the next section.</p>
<p>To start out, let’s generate, yet again, a simple self-interaction matrix for a free surface. For simplicity, I just used a triangulated planar rectangle rather than anything realistic. I hid these cells since the code is nothing new.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;

<span class="kn">import</span> <span class="nn">cutde</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surf_L</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="n">n_els_per_dim</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">mesh_xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">surf_L</span><span class="p">,</span> <span class="n">surf_L</span><span class="p">,</span> <span class="n">n_els_per_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">mesh_ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">surf_L</span><span class="p">,</span> <span class="n">surf_L</span><span class="p">,</span> <span class="n">n_els_per_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">mesh_xg</span><span class="p">,</span> <span class="n">mesh_yg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">mesh_xs</span><span class="p">,</span> <span class="n">mesh_ys</span><span class="p">)</span>
<span class="n">surf_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mesh_xg</span><span class="p">,</span> <span class="n">mesh_yg</span><span class="p">,</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">mesh_yg</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">surf_tris</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nx</span> <span class="o">=</span> <span class="n">ny</span> <span class="o">=</span> <span class="n">n_els_per_dim</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">idx</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="n">ny</span> <span class="o">+</span> <span class="n">j</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_els_per_dim</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_els_per_dim</span><span class="p">):</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">mesh_xs</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">mesh_ys</span><span class="p">[</span><span class="n">j</span> <span class="p">:</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">surf_tris</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">idx</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">idx</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">idx</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
        <span class="n">surf_tris</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">idx</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">idx</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">idx</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

<span class="n">surf_tris</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">surf_tris</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">surf_tri_pts</span> <span class="o">=</span> <span class="n">surf_pts</span><span class="p">[</span><span class="n">surf_tris</span><span class="p">]</span>
<span class="n">surf_centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">surf_tri_pts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">surf_surf_mat</span> <span class="o">=</span> <span class="n">cutde</span><span class="o">.</span><span class="n">disp_matrix</span><span class="p">(</span>
    <span class="n">surf_centroids</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]),</span> <span class="n">surf_pts</span><span class="p">[</span><span class="n">surf_tris</span><span class="p">],</span> <span class="mf">0.25</span>
<span class="p">)</span>

<span class="n">lhs_reordered</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">surf_surf_mat</span><span class="p">)</span>
<span class="n">lhs_reordered</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">surf_surf_mat</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">lhs_reordered</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">surf_surf_mat</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">lhs_reordered</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">surf_surf_mat</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">lhs_reordered</span> <span class="o">=</span> <span class="n">lhs_reordered</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">surf_tris</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">surf_tris</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">lhs_reordered</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">lhs_reordered</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">lhs_reordered</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="near-field-vs-far-field">
<h2><span class="section-number">4.1. </span>Near-field vs far-field<a class="headerlink" href="#near-field-vs-far-field" title="Permalink to this headline">¶</a></h2>
<p>Let’s dig in and investigate the matrix itself! In particular, let’s start by looking at two blocks of the matrix.</p>
<ol class="simple">
<li><p>A “near-field” block will contain the diagonal of the matrix. Remember that the diagonal of the matrix consists of entries representing the displacement at the center of the same element on which the slip occurred. You can see the bright yellow diagonal in the figure below. The matrix entries decay rapidly away from the diagonal.</p></li>
<li><p>The other, a “far-field” block will consist of matrix entries coming from interactions between observation points and source elements that are very far from each other. In the figure below, there’s no intense variation in entries.</p></li>
</ol>
<p>The thing to notice here is that there is, in some sense, just a lot more going on in the near-field matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nrows</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">near_field_block</span> <span class="o">=</span> <span class="n">A</span><span class="p">[:</span><span class="n">nrows</span><span class="p">,</span> <span class="p">:</span><span class="n">nrows</span><span class="p">]</span>
<span class="n">far_field_block</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="n">nrows</span><span class="p">:,</span> <span class="p">:</span><span class="n">nrows</span><span class="p">]</span>

<span class="n">log_near</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">near_field_block</span><span class="p">)</span>
<span class="n">log_near</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">log_near</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>
<span class="n">log_far</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">far_field_block</span><span class="p">)</span>
<span class="n">log_far</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">log_far</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">log_near</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Near-field&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ims</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">log_far</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Far-field&quot;</span><span class="p">)</span>
<span class="n">cbar_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">0.935</span><span class="p">,</span> <span class="mf">0.125</span><span class="p">,</span> <span class="mf">0.015</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
<span class="n">cb</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ims</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cbar_ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-3-38c45f4a2f2b&gt;:5: RuntimeWarning: invalid value encountered in log10
  log_near = np.log10(near_field_block)
&lt;ipython-input-3-38c45f4a2f2b&gt;:7: RuntimeWarning: invalid value encountered in log10
  log_far = np.log10(far_field_block)
</pre></div>
</div>
<img alt="../_images/low_rank_4_1.png" src="../_images/low_rank_4_1.png" />
</div>
</div>
<p>Another way of visualizing this same “a lot going on” property would be to look at the action of the matrix. So, we’ll apply both the nearfield and far-field blocks to a vector with random elements. The difference is striking: the output from the near-field matrix preserves the “randomness” of the input whereas the far-field matrix actually smooths out the input dramatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nrows</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">near_field_block</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">far_field_block</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y1</span><span class="p">[</span><span class="mi">2</span><span class="p">::</span><span class="mi">3</span><span class="p">])))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Near-field&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y2</span><span class="p">[</span><span class="mi">2</span><span class="p">::</span><span class="mi">3</span><span class="p">])))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Far-field&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/low_rank_6_0.png" src="../_images/low_rank_6_0.png" />
</div>
</div>
<p>A third and more rigorous way of discussing this same property is to look at the singular values of the two matrix blocks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U_near</span><span class="p">,</span> <span class="n">S_near</span><span class="p">,</span> <span class="n">V_near</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">near_field_block</span><span class="p">)</span>
<span class="n">U_far</span><span class="p">,</span> <span class="n">S_far</span><span class="p">,</span> <span class="n">V_far</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">far_field_block</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The plot below shows the <span class="math notranslate nohighlight">\(\log_{10}\)</span> ratio of each singular value to the first singular value. Things to note here:</p>
<ul class="simple">
<li><p>The near-field singular values do not decay. They are almost all between 1.58 and 1.41. This reflects the behavior above where the randomness of the <code class="docutils literal notranslate"><span class="pre">x</span></code> vector was mostly preserved.</p></li>
<li><p>The far-field singular values decay very quickly. The majority of the singular values are smaller than 1e-6. In other words, the matrix has a “1e-6 approximate rank” of 14 despite technically have a rank of 150. That means that if we only care about accuracy up to about 6 digits, then we can compress this matrix by about a factor of 6.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">S_near</span> <span class="o">/</span> <span class="n">S_near</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;nearfield&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">S_far</span> <span class="o">/</span> <span class="n">S_far</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;farfield&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/low_rank_10_0.png" src="../_images/low_rank_10_0.png" />
</div>
</div>
<p>The 14th far-field singular value is below 1e-6.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">S_far</span><span class="p">[</span><span class="mi">14</span><span class="p">]</span> <span class="o">/</span> <span class="n">S_far</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.8980941903866048e-07
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="approximation-with-the-singular-value-decomposition-svd">
<h2><span class="section-number">4.2. </span>Approximation with the singular value decomposition (SVD)<a class="headerlink" href="#approximation-with-the-singular-value-decomposition-svd" title="Permalink to this headline">¶</a></h2>
<p>So, if these off-diagonal blocks of the matrix have singular values that decay quite quickly, can we approximate these blocks with just a few singular values? The answer is an emphatic yes and this idea is the basis of low-rank approximation methods. Let’s explore this idea a bit more with a larger 3000x3000 block coming the lower left corner of the matrix. Because it’s also off-diagonal, this block shows the same singular value magnitude decay that we saw before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">3000</span><span class="p">:,</span> <span class="p">:</span><span class="mi">3000</span><span class="p">]</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">S</span> <span class="o">/</span> <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/low_rank_14_0.png" src="../_images/low_rank_14_0.png" />
</div>
</div>
<p>To continue, it’s useful to define what the relevant metric for the matrix approximation that we’re going to do. A useful norm is the Frobenius norm because it depends only on the matrix entries and for a matrix <span class="math notranslate nohighlight">\(M \in \mathbb{R}^{n x m}\)</span> is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-834d0da4-6b72-4bfa-8a27-b3fd8c3bf4c2">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-834d0da4-6b72-4bfa-8a27-b3fd8c3bf4c2" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\|M\|_F = \sqrt{\sum_{i,j}^{m,n} M_{ij}^2}
\end{equation}\]</div>
<p>An important fact is that the Frobenius norm of a matrix is the root of the sum of the square of the singular values. That is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-87a1000e-9253-42e0-8849-69426a6c3a4d">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-87a1000e-9253-42e0-8849-69426a6c3a4d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\|M\|_F = \sqrt{\sum_{k}^{min(m,n)} \sigma_k^2}
\end{equation}\]</div>
<p>So, what if we want to approximate a matrix with a particular tolerance, <span class="math notranslate nohighlight">\(\epsilon\)</span> in terms of the Frobenius norm? One way to do it will be to compute the SVD and then find the lowest index <span class="math notranslate nohighlight">\(K\)</span> for which the constraint is true:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3df6f7c7-1345-4985-af9c-22610c2715ad">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-3df6f7c7-1345-4985-af9c-22610c2715ad" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\sqrt{\sum_{k=1}^K \sigma_k^2} - \|M\|_F = \sqrt{\sum_{k=K}^{min(m,n)} \sigma_k^2} &lt; \epsilon
\end{equation}\]</div>
<p>The middle sum from <span class="math notranslate nohighlight">\(K\)</span> to the number of rows/columns is essentially a measure of how important the dropped singular values are. So we want that sum to be smaller than our tolerance! The actual implementation below is surprisingly simple.</p>
<p>Here, I’ll choose <span class="math notranslate nohighlight">\(\epsilon = 10^{-8}\)</span>. As demonstrated below, we only need the first 40 singular values to compute such an approximation, meaning that we can store the left and right singular vectors in matrices of shape <code class="docutils literal notranslate"><span class="pre">(3000,</span> <span class="pre">40)</span></code>. This will compress the matrix by a factor of 37.5x!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-8</span>
<span class="n">eps</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1e-08
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reverse the list of singular values and sum them to compute the</span>
<span class="c1"># error from each level of truncation.</span>
<span class="n">frob_K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">S</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">appx_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">frob_K</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span>
<span class="n">appx_rank</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>40
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">frob_K</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1.03496590e-02, 7.73331069e-03, 3.53462239e-03, ..., 2.62094754e-20, 1.47210395e-20, 4.74891493e-21])
</pre></div>
</div>
</div>
</div>
<p>And let’s take a look at the performance and error resulting from applying this low rank approximation to a random vector. First, I’ll create an random input vector <code class="docutils literal notranslate"><span class="pre">x</span></code>. Then, I’ll form two matrices <code class="docutils literal notranslate"><span class="pre">Uappx</span></code> and <code class="docutils literal notranslate"><span class="pre">Vappx</span></code>. You can sort of think of these as the entrance and exit to our low-dimensional space. This low-dimensional space allows for a very computationally efficient representation of the TDE interactions. Mutliplying <code class="docutils literal notranslate"><span class="pre">Vappx</span></code> by the 3000-dimensional <code class="docutils literal notranslate"><span class="pre">x</span></code> returns a 40-dimensional vector and then we expand back to 3000 dimensional by multiplying by <code class="docutils literal notranslate"><span class="pre">Uappx</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Uappx</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">appx_rank</span><span class="p">]</span>
<span class="n">Vappx</span> <span class="o">=</span> <span class="n">S</span><span class="p">[:</span><span class="n">appx_rank</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">V</span><span class="p">[:</span><span class="n">appx_rank</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Next, I’ll calculate the correct matrix vector product, <code class="docutils literal notranslate"><span class="pre">y_true</span> <span class="pre">=</span> <span class="pre">block.dot(x)</span></code> and record the runtime.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_time</span> <span class="o">=</span> <span class="o">%</span><span class="k">timeit</span> -o block.dot(x)
<span class="n">y_true</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15.8 ms ± 355 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</pre></div>
</div>
</div>
</div>
<p>And then, calculate the low-rank matrix vector product, <code class="docutils literal notranslate"><span class="pre">y_appx</span></code> and record the runtime.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lowrank_time</span> <span class="o">=</span> <span class="o">%</span><span class="k">timeit</span> -o Uappx.dot(Vappx.dot(x))
<span class="n">y_appx</span> <span class="o">=</span> <span class="n">Uappx</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Vappx</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>114 µs ± 3.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</pre></div>
</div>
</div>
</div>
<p>A cursory comparison of the output suggests that the approximation is very very accurate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_true</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_appx</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.00156452 -0.00307875  0.00146438  0.00158299 -0.00314753] [ 0.00156452 -0.00307875  0.00146438  0.00158299 -0.00314753]
</pre></div>
</div>
</div>
</div>
<p>And the calculation is far, far faster!!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">speedup</span> <span class="o">=</span> <span class="n">full_time</span><span class="o">.</span><span class="n">best</span> <span class="o">/</span> <span class="n">lowrank_time</span><span class="o">.</span><span class="n">best</span>
<span class="n">memory_reduction</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="n">Uappx</span><span class="o">.</span><span class="n">nbytes</span> <span class="o">+</span> <span class="n">Vappx</span><span class="o">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;speed up         = </span><span class="si">{</span><span class="n">speedup</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;memory reduction = </span><span class="si">{</span><span class="n">memory_reduction</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>speed up         = 141.24925308518618
memory reduction = 37.5
</pre></div>
</div>
</div>
</div>
<p>Let’s do a bit more detailed investigation of the error and look at the <span class="math notranslate nohighlight">\(L^1\)</span>, <span class="math notranslate nohighlight">\(L^2\)</span> and <span class="math notranslate nohighlight">\(L^{\infty}\)</span> relative error in the matrix vector product. We’ll also compare the Frobenius norm of the approximated matrix with the Frobenius norm of the original matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">abs_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
<span class="n">l2_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_true</span><span class="p">)</span>
<span class="n">l1_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_true</span><span class="p">)</span>
<span class="n">linf_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">abs_true</span><span class="p">)</span>

<span class="n">abs_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_appx</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">l2_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_diff</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">l2_true</span>
<span class="n">l1_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">l1_true</span>
<span class="n">linf_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">linf_true</span>
<span class="n">true_frob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">block</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">frob_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">Uappx</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Vappx</span><span class="p">)</span> <span class="o">-</span> <span class="n">block</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L1(UVx-y)        = </span><span class="si">{</span><span class="n">l1_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L2(UVx-y)        = </span><span class="si">{</span><span class="n">l2_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Linf(UVx-y)      = </span><span class="si">{</span><span class="n">linf_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Frobenius(M-UV)  = </span><span class="si">{</span><span class="n">frob_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L1(UVx-y)        = 1.1535850312628712e-08
L2(UVx-y)        = 1.5729499549438905e-08
Linf(UVx-y)      = 4.840348614253369e-08
Frobenius(M-UV)  = 8.902174079271246e-09
</pre></div>
</div>
</div>
</div>
<p>It looks like this SVD approximation worked out really well here! We’re getting very low error matrix-vector products. These errors are all somewhat similar to threshold we used for the singular value cutoff. And, based on the Frobenius error, the approximate matrix itself is extremely similar to the original matrix. The Frobenius error is just barely better than our tolerance condition, as expected.</p>
<p>Before we move on, I’ll record these error values in a dataframe so that it’s easy to compare with the fancier methods in the next two sections.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">err_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Rank&quot;</span><span class="p">,</span> <span class="s2">&quot;L2(UVx-y)&quot;</span><span class="p">,</span> <span class="s2">&quot;L1(UVx-y)&quot;</span><span class="p">,</span> <span class="s2">&quot;Linf(UVx-y)&quot;</span><span class="p">,</span> <span class="s2">&quot;Frobenius(M-UV)&quot;</span><span class="p">],</span>
    <span class="n">data</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">true</span><span class="o">=</span><span class="p">[</span><span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span>
<span class="p">)</span>
<span class="n">err_df</span><span class="p">[</span><span class="s2">&quot;SVD&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">appx_rank</span><span class="p">,</span> <span class="n">l2_err</span><span class="p">,</span> <span class="n">l1_err</span><span class="p">,</span> <span class="n">linf_err</span><span class="p">,</span> <span class="n">frob_err</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">err_df</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Rank</th>
      <th>L2(UVx-y)</th>
      <th>L1(UVx-y)</th>
      <th>Linf(UVx-y)</th>
      <th>Frobenius(M-UV)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>true</th>
      <td>3000.0</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
    </tr>
    <tr>
      <th>SVD</th>
      <td>40.0</td>
      <td>1.572950e-08</td>
      <td>1.153585e-08</td>
      <td>4.840349e-08</td>
      <td>8.902174e-09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="adaptive-cross-approximation-aca">
<h2><span class="section-number">4.3. </span>Adaptive cross approximation (ACA)<a class="headerlink" href="#adaptive-cross-approximation-aca" title="Permalink to this headline">¶</a></h2>
<p>So, we’ve managed to create an extremely efficient approximation of our off-diagonal matrix block by using the SVD. That’s definitely useful for computing fast matrix-vector products or even for computing a LU decomposition. But, it still suffers from the need to compute the entire matrix block in the first place, an <span class="math notranslate nohighlight">\(O(n^2)\)</span> operation! If we’re going to be throwing all that information away immediately after computing the SVD, is there a way to avoid computing the dense matrix block in the first place? There are several solutions to this problem like randomized SVDs <span id="id4">[<a class="reference internal" href="../references.html#id440">Halko <em>et al.</em>, 2010</a>]</span>, but the most useful solution for our setting is the adaptive cross approximation (ACA) method <span id="id5">[<a class="reference internal" href="../references.html#id81">Bebendorf and Rjasanow, 2003</a>, <a class="reference internal" href="../references.html#id82">Bebendorf, 2000</a>]</span>. These algorithms depending on the ability to compute arbitrary individual matrix entries without computing the entire matrix. By making certain assumptions about the structure of a matrix, we can be confident that an accurate approximation can be constructed from just a few rows and columns.</p>
<p>The basic idea of ACA is to approximate a matrix with a rank 1 outer product of one row and one column of that same matrix. And then iteratively use this process to construct an approximation of arbitrary precision. Ideally, at each step, we will choose the best fitting row and column. After the first iteration, we are no longer trying to approximate the original matrix, but instead we approximate the residual matrix formed by the difference between the original matrix and the current approximation matrix. Eventually, given certain matrix properties that have been proven true for BEM problem  <span id="id6">[<a class="reference internal" href="../references.html#id81">Bebendorf and Rjasanow, 2003</a>]</span>, the procedure will converge.</p>
<p>For the sake of real-world usage, the description above will be sufficient for understanding what’s going on. But, if you want to dive into a detailed description and implementation of the method, see below! Otherwise, skip over the next two sub-sections.</p>
<div class="section" id="aca-and-aca-the-details">
<h3><span class="section-number">4.3.1. </span>ACA and ACA+, the details<a class="headerlink" href="#aca-and-aca-the-details" title="Permalink to this headline">¶</a></h3>
<p>The simple version, runs like (following <span id="id7">Grasedyck [<a class="reference internal" href="../references.html#id406">2005</a>]</span>):</p>
<p><strong>ACA with full pivoting</strong>: Given a matrix <span class="math notranslate nohighlight">\(M \in \mathbb{R}^{n x m}\)</span>, we’ll construct an approximation like <span class="math notranslate nohighlight">\(\sum_{k}^{r} u_k v_k^T\)</span>. The task will be to construct the row and column vectors <span class="math notranslate nohighlight">\(u_k\)</span> and <span class="math notranslate nohighlight">\(v_k\)</span> on each iteration such that the Frobenius norm error eventually converges. To do that, the key will be to iteratively form rank-1 approximations to the residual matrix. The residual matrix is the matrix forming the difference between <span class="math notranslate nohighlight">\(M\)</span> and the current approximation and can be written as <span class="math notranslate nohighlight">\(R_{ij} = M_{ij} - \sum_{k}^{r} u_{kj} v_{ki}\)</span> representing the entry-wise difference between the target matrix and the current approximation. The goal will be to have <span class="math notranslate nohighlight">\(R\)</span> satisfy <span class="math notranslate nohighlight">\(\|R\|_2 &lt; \epsilon\)</span> where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a user-specified accuracy parameter. To do this, during each iteration:</p>
<ol>
<li><p>Determine a “pivot” <span class="math notranslate nohighlight">\((i^*, j^*)\)</span> as the indices that maximize <span class="math notranslate nohighlight">\(R_{ij}\)</span>. Intuitively, the largest rows and columns are going to be most “important”.</p></li>
<li><p>Assign a new rank-1 component of the approximation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-80516195-c96f-49ab-9116-d9a106ac8d1d">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-80516195-c96f-49ab-9116-d9a106ac8d1d" title="Permalink to this equation">¶</a></span>\[\begin{align}
    u_{kj} &amp;= R_{i^*,j} / R_{i^*,j^*} \\
    v_{ki} &amp;= R_{i,j^*}
    \end{align}\]</div>
</li>
<li><p>Update <span class="math notranslate nohighlight">\(R\)</span> to account for the new rank-1 update to the approximation.</p></li>
<li><p>If the magnitude of the <span class="math notranslate nohighlight">\(u_k v_k\)</span> update is small enough, stop. Otherwise, return to step 1.</p></li>
</ol>
<p>The reason the algorithm is called “ACA with full pivoting” is because we’re allowing the algorithm to choose an arbitrary <span class="math notranslate nohighlight">\((i^*, j^*)\)</span> in the first step. However, that is impossible in our real-world setting because it requires full knowledge of the residual matrix. But, we don’t have all the entries of <span class="math notranslate nohighlight">\(M\)</span> so we can’t have all the entries of <span class="math notranslate nohighlight">\(R_{ij}\)</span>. Computing all the entries of <span class="math notranslate nohighlight">\(M\)</span> is exactly what we’re trying to avoid! But, this basic “full information” algorithm is instructive and can be modified slightly so that we don’t need the whole matrix.</p>
<p><strong>ACA+</strong>: Most real-world application use either <em>ACA with partial pivoting</em> or the <em>ACA+</em> (“ACA plus”) algorithm. Here, I’ll introduce the modifications necessary for ACA+. The main distinction is that instead of searching over all matrix indices in step 1, we will search over a subset specified by a random row and a random row. This row and column will be our window into the matrix. Instead of finding the largest entry across the full pivoting algorithm in step 1, we will just find the largest entry in either the row window or the column window. While the ACA+ approximation will be less efficient and will converge more slowly than ACA with full pivoting, the end result will still be very good. Note that this approach may completely fail for general low rank matrices, but is very successful for the low rank matrices specifically coming from BEM problems.</p>
<p>The algorithm: Before starting the iteration, we choose a random row, <span class="math notranslate nohighlight">\(i_{\mathrm{ref}}\)</span> and random <span class="math notranslate nohighlight">\(j_{\mathrm{ref}}\)</span>. And we will maintain the corresponding row and column of the residual matrix, <span class="math notranslate nohighlight">\(R\)</span>. At the start of the algorithm <span class="math notranslate nohighlight">\(R_{i_{\mathrm{ref}}, j} = M_{i_{\mathrm{ref}}, j}\)</span> and <span class="math notranslate nohighlight">\(R_{i, j_{\mathrm{ref}}} = M_{i, _{\mathrm{ref}}j}\)</span></p>
<p>Then, the iteration proceeds like:</p>
<ol>
<li><p>Find the index <span class="math notranslate nohighlight">\(j^*\)</span> that maximizes <span class="math notranslate nohighlight">\(R_{i_{\mathrm{ref}}, j}\)</span> and the index <span class="math notranslate nohighlight">\(i^*\)</span> that maximizes <span class="math notranslate nohighlight">\(R_{i, j_{\mathrm{ref}}}\)</span>. Essentially, we are finding the largest entries in each of these vectors.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(R_{i_{\mathrm{ref}}, j^*} &gt; R_{i^*, j_{\mathrm{ref}}}\)</span> then, we compute the column corresponding to <span class="math notranslate nohighlight">\(j^*\)</span> or if the opposite is true, we compute the row corresponding to <span class="math notranslate nohighlight">\(i^*\)</span>. Essentially, we are determining here whether we should pivot first based on the row or the column.</p></li>
<li><p>If we pivoted based on column, we should now have computed a new residual column <span class="math notranslate nohighlight">\(R_{i,j^*} = M_{i,j^*} - \sum_{k}^{r} u_{kj^*} v_{ki}\)</span>. Find the missing pivot index now by maximizing <span class="math notranslate nohighlight">\(R_{i,j^*}\)</span> to get <span class="math notranslate nohighlight">\(i^*\)</span>. Or if we pivoted on the row, we will have a new residual row <span class="math notranslate nohighlight">\(R_{i^*,j} = M_{i^*,j} - \sum_{k}^{r} u_{kj} v_{ki^*}\)</span> and we maximize <span class="math notranslate nohighlight">\(R_{i^*,j}\)</span> to get <span class="math notranslate nohighlight">\(j^*\)</span>. The idea here is to finish the pivot operation from step 2 by pivoting in the dimension that we have not considered yet. At each step, we are essentially trying to find the largest residual matrix element out of all the entries we have seen so far with the goal of getting as close as possible to the full pivoting algorithm without actually needing to calculate all the residual matrix elements. Why am I referring to the identification of the largest entries as “pivoting”? This is by analogy to various matrix operations like LU decomposition where the numerical stability is best when the largest entries are handled first. ACA can also be reframed as an iterative triangular decomposition of a matrix. Before moving on, note how we calculate the residual row (column) here by computing the original matrix row and then subtracting the row (column) of the current approximation. This is critical since it means that we’re only compunting a single row or column of the original matrix.</p></li>
<li><p>Next, just like in the full pivoting algorithm, assign:</p>
<div class="amsmath math notranslate nohighlight" id="equation-880eb617-58a6-4418-aa0f-75a73af45aac">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-880eb617-58a6-4418-aa0f-75a73af45aac" title="Permalink to this equation">¶</a></span>\[\begin{align}
    u_{kj} &amp;= R_{i^*,j} / R_{i^*,j^*} \\
    v_{ki} &amp;= R_{i,j^*}
    \end{align}\]</div>
</li>
<li><p>And update the <span class="math notranslate nohighlight">\(R_{i_{\mathrm{ref}}, j}\)</span> row and <span class="math notranslate nohighlight">\(R_{i, j_{\mathrm{ref}}}\)</span> by subtracting the new terms of the approximation.</p></li>
<li><p>Finally, if the magnitude of the <span class="math notranslate nohighlight">\(u_k v_k\)</span> update is small enough, stop. Otherwise, return to step 1.</p></li>
</ol>
<p>I hope the key difference with the full pivoting algorithm is now clear – we only need to compute a single row and a single column of the matrix for each iteration. If the algorithm converges in a number of iterations less than the rank of the matrix <span class="math notranslate nohighlight">\(M\)</span>, then we will have computed only a small subset of the entries. I’ve deliberately left some of the details vague in order to make the salient features of the algorithm more prominent. But, below, I’m going to go through a full implementation of the algorithm so hopefully that will clear up any of the details.</p>
</div>
<div class="section" id="implementing-aca">
<h3><span class="section-number">4.3.2. </span>Implementing ACA+<a class="headerlink" href="#implementing-aca" title="Permalink to this headline">¶</a></h3>
<p>An implementation of ACA+ is below. I’ve put lots of comments throughout to help explain the details. But, if you don’t want to dive into the details here, just skip over this section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ACA_plus</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">calc_rows</span><span class="p">,</span> <span class="n">calc_cols</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run the ACA+ plus algorithm on a matrix implicitly defined by the</span>
<span class="sd">    row and column computation functions passed as arguments.</span>

<span class="sd">    :param n_rows:</span>
<span class="sd">    :param n_cols:</span>
<span class="sd">    :param calc_rows: A function that accepts two parameters (Istart, Iend)</span>
<span class="sd">        specifying the first and last row desired and returns a numpy array</span>
<span class="sd">        with shape (Iend-Istart, N_col) with the corresponding rows of the</span>
<span class="sd">        input matrix</span>
<span class="sd">    :param calc_cols: A function that accepts two parameters (Jstart, Jend)</span>
<span class="sd">        specifying the first and last column desired and returns a numpy array</span>
<span class="sd">        with shape (N_rows, Jend-Jstart) with the corresponding columns of the</span>
<span class="sd">        input matrix</span>
<span class="sd">    :param eps: The tolerance of the approximation. The convergence condition is</span>
<span class="sd">        in terms of the difference in Frobenius norm between the target matrix</span>
<span class="sd">        and the approximation</span>
<span class="sd">    :param max_iter:</span>
<span class="sd">    :param verbose: Should we print information at each iteration. Just included</span>
<span class="sd">        for demonstration here.</span>

<span class="sd">    :return U_ACA: The left-hand approximation matrix.</span>
<span class="sd">    :return V_ACA: The right-hand approximation matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">us</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># The left vectors of the approximation</span>
    <span class="n">vs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># The right vectors of the approximation</span>
    <span class="n">prevIstar</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Previously used i^* pivots</span>
    <span class="n">prevJstar</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Previously used j^* pivots</span>

    <span class="c1"># a quick helper function that will help find the largest entry in</span>
    <span class="c1"># an array while excluding some list of `disallowed`  entries.</span>
    <span class="k">def</span> <span class="nf">argmax_not_in_list</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">disallowed</span><span class="p">):</span>
        <span class="n">arg_sorted</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
        <span class="n">max_idx</span> <span class="o">=</span> <span class="n">arg_sorted</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arg_sorted</span><span class="p">[</span><span class="n">max_idx</span><span class="p">]</span> <span class="ow">in</span> <span class="n">disallowed</span><span class="p">:</span>
                <span class="n">max_idx</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="n">arg_sorted</span><span class="p">[</span><span class="n">max_idx</span><span class="p">]</span>

    <span class="c1"># A function that will return a contiguous block of rows of the</span>
    <span class="c1"># residual matrix</span>
    <span class="k">def</span> <span class="nf">calc_residual_rows</span><span class="p">(</span><span class="n">Istart</span><span class="p">,</span> <span class="n">Iend</span><span class="p">):</span>
        <span class="c1"># First calculate the rows of the original matrix.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">calc_rows</span><span class="p">(</span><span class="n">Istart</span><span class="p">,</span> <span class="n">Iend</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Then subtract the current terms of the approximation</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">us</span><span class="p">)):</span>
            <span class="n">out</span> <span class="o">-=</span> <span class="n">us</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">Istart</span><span class="p">:</span><span class="n">Iend</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">vs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># See above, except this function calculates a block of columns.</span>
    <span class="k">def</span> <span class="nf">calc_residual_cols</span><span class="p">(</span><span class="n">Jstart</span><span class="p">,</span> <span class="n">Jend</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">calc_cols</span><span class="p">(</span><span class="n">Jstart</span><span class="p">,</span> <span class="n">Jend</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">us</span><span class="p">)):</span>
            <span class="n">out</span> <span class="o">-=</span> <span class="n">vs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">Jstart</span><span class="p">:</span><span class="n">Jend</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">us</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># A function for finding a reference row and updating</span>
    <span class="c1"># it with respect to the already constructed approximation.</span>
    <span class="k">def</span> <span class="nf">reset_reference_row</span><span class="p">(</span><span class="n">Iref</span><span class="p">):</span>
        <span class="c1"># When a row gets used in the approximation, we will need to</span>
        <span class="c1"># reset to use a different reference row. Just, increment!</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">Iref</span> <span class="o">=</span> <span class="p">(</span><span class="n">Iref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_rows</span>
            <span class="n">Iref</span> <span class="o">-=</span> <span class="n">Iref</span> <span class="o">%</span> <span class="mi">3</span>
            <span class="k">if</span> <span class="n">Iref</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">prevIstar</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># Grab the &quot;row&quot; (actually three rows corresponding to the</span>
        <span class="c1"># x, y, and z components for a single observation point)</span>
        <span class="k">return</span> <span class="n">calc_residual_rows</span><span class="p">(</span><span class="n">Iref</span><span class="p">,</span> <span class="n">Iref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">),</span> <span class="n">Iref</span>

    <span class="c1"># Same function as above but for the reference column</span>
    <span class="k">def</span> <span class="nf">reset_reference_col</span><span class="p">(</span><span class="n">Jref</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">Jref</span> <span class="o">=</span> <span class="p">(</span><span class="n">Jref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_cols</span>
            <span class="n">Jref</span> <span class="o">-=</span> <span class="n">Jref</span> <span class="o">%</span> <span class="mi">3</span>
            <span class="k">if</span> <span class="n">Jref</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">prevJstar</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">calc_residual_cols</span><span class="p">(</span><span class="n">Jref</span><span class="p">,</span> <span class="n">Jref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">),</span> <span class="n">Jref</span>

    <span class="c1"># If we haven&#39;t converged before running for max_iter, we&#39;ll stop anyway.</span>
    <span class="k">if</span> <span class="n">max_iter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_iter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_iter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">])</span>

    <span class="c1"># Create a buffer for storing the R_{i^*,j} and R_{i, j^*}</span>
    <span class="n">RIstar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_cols</span><span class="p">)</span>
    <span class="n">RJstar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_rows</span><span class="p">)</span>

    <span class="c1"># Choose our starting random reference row and column.</span>
    <span class="c1"># These will get incremented by 3 inside reset_reference_row</span>
    <span class="c1"># so pre-subtract that.</span>
    <span class="n">Iref</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">n_rows</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>
    <span class="n">Jref</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">n_cols</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>
    <span class="c1"># And collect the corresponding blocks of rows/columns</span>
    <span class="n">RIref</span><span class="p">,</span> <span class="n">Iref</span> <span class="o">=</span> <span class="n">reset_reference_row</span><span class="p">(</span><span class="n">Iref</span><span class="p">)</span>
    <span class="n">RJref</span><span class="p">,</span> <span class="n">Jref</span> <span class="o">=</span> <span class="n">reset_reference_col</span><span class="p">(</span><span class="n">Jref</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="c1"># These two lines find the column in RIref with the largest entry (step 1 above).</span>
        <span class="n">maxabsRIref</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">RIref</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Jstar</span> <span class="o">=</span> <span class="n">argmax_not_in_list</span><span class="p">(</span><span class="n">maxabsRIref</span><span class="p">,</span> <span class="n">prevJstar</span><span class="p">)</span>

        <span class="c1"># And these two find the row in RJref with the largest entry (step 1 above).</span>
        <span class="n">maxabsRJref</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">RJref</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">Istar</span> <span class="o">=</span> <span class="n">argmax_not_in_list</span><span class="p">(</span><span class="n">maxabsRJref</span><span class="p">,</span> <span class="n">prevIstar</span><span class="p">)</span>

        <span class="c1"># Check if we should pivot first based on row or based on column (step 2 above)</span>
        <span class="n">Jstar_val</span> <span class="o">=</span> <span class="n">maxabsRIref</span><span class="p">[</span><span class="n">Jstar</span><span class="p">]</span>
        <span class="n">Istar_val</span> <span class="o">=</span> <span class="n">maxabsRJref</span><span class="p">[</span><span class="n">Istar</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">Istar_val</span> <span class="o">&gt;</span> <span class="n">Jstar_val</span><span class="p">:</span>
            <span class="c1"># If we pivot first on the row, then calculate the corresponding row</span>
            <span class="c1"># of the residual matrix.</span>
            <span class="n">RIstar</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">calc_residual_rows</span><span class="p">(</span><span class="n">Istar</span><span class="p">,</span> <span class="n">Istar</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Then find the largest entry in that row vector to identify which</span>
            <span class="c1"># column to pivot on. (See step 3 above)</span>
            <span class="n">Jstar</span> <span class="o">=</span> <span class="n">argmax_not_in_list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">RIstar</span><span class="p">),</span> <span class="n">prevJstar</span><span class="p">)</span>

            <span class="c1"># Calculate the corresponding residual column!</span>
            <span class="n">RJstar</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">calc_residual_cols</span><span class="p">(</span><span class="n">Jstar</span><span class="p">,</span> <span class="n">Jstar</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If we pivot first on the column, then calculate the corresponding column</span>
            <span class="c1"># of the residual matrix.</span>
            <span class="n">RJstar</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">calc_residual_cols</span><span class="p">(</span><span class="n">Jstar</span><span class="p">,</span> <span class="n">Jstar</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Then find the largest entry in that row vector to identify which</span>
            <span class="c1"># column to pivot on.  (See step 3 above)</span>
            <span class="n">Istar</span> <span class="o">=</span> <span class="n">argmax_not_in_list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">RJstar</span><span class="p">),</span> <span class="n">prevIstar</span><span class="p">)</span>

            <span class="c1"># Calculate the corresponding residual row!</span>
            <span class="n">RIstar</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">calc_residual_rows</span><span class="p">(</span><span class="n">Istar</span><span class="p">,</span> <span class="n">Istar</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Record the pivot row and column so that we don&#39;t re-use them.</span>
        <span class="n">prevIstar</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Istar</span><span class="p">)</span>
        <span class="n">prevJstar</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Jstar</span><span class="p">)</span>

        <span class="c1"># Add the new rank-1 outer product to the approximation (see step 4 above)</span>
        <span class="n">vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RIstar</span> <span class="o">/</span> <span class="n">RIstar</span><span class="p">[</span><span class="n">Jstar</span><span class="p">])</span>
        <span class="n">us</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RJstar</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="c1"># How &quot;large&quot; was this update to the approximation?</span>
        <span class="n">step_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">us</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;pivot row=</span><span class="si">{</span><span class="n">Istar</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2">, pivot col=</span><span class="si">{</span><span class="n">Jstar</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;step size=</span><span class="si">{</span><span class="n">step_size</span><span class="si">:</span><span class="s2">1.3e</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;tolerance=</span><span class="si">{</span><span class="n">eps</span><span class="si">:</span><span class="s2">1.3e</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># The convergence criteria will simply be whether the Frobenius norm of the</span>
        <span class="c1"># step is smaller than the user provided tolerance.</span>
        <span class="k">if</span> <span class="n">step_size</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># We also break here if this is the last iteration to avoid wasting effort</span>
        <span class="c1"># updating the reference row/column</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">max_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># If we didn&#39;t converge, let&#39;s prep the reference residual row and</span>
        <span class="c1"># column for the next iteration:</span>

        <span class="c1"># If we pivoted on the reference row, then choose a new reference row.</span>
        <span class="c1"># Remember that we are using a x,y,z vector &quot;row&quot; or</span>
        <span class="c1"># set of 3 rows in an algebraic sense.</span>
        <span class="k">if</span> <span class="n">Iref</span> <span class="o">&lt;=</span> <span class="n">Istar</span> <span class="o">&lt;</span> <span class="n">Iref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">RIref</span><span class="p">,</span> <span class="n">Iref</span> <span class="o">=</span> <span class="n">reset_reference_row</span><span class="p">(</span><span class="n">Iref</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If we didn&#39;t change the reference row of the residual matrix &quot;R&quot;,</span>
            <span class="c1"># update the row to account for the new components of the approximation.</span>
            <span class="n">RIref</span> <span class="o">-=</span> <span class="n">us</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">Iref</span> <span class="p">:</span> <span class="n">Iref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">vs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># If we pivoted on the reference column, then choose a new reference column.</span>
        <span class="c1"># Remember that we are using a x,y,z vector &quot;column&quot; or</span>
        <span class="c1"># set of 3 columns in an algebraic sense.</span>
        <span class="k">if</span> <span class="n">Jref</span> <span class="o">&lt;=</span> <span class="n">Jstar</span> <span class="o">&lt;</span> <span class="n">Jref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">RJref</span><span class="p">,</span> <span class="n">Jref</span> <span class="o">=</span> <span class="n">reset_reference_col</span><span class="p">(</span><span class="n">Jref</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If we didn&#39;t change the reference column of the residual matrix &quot;R&quot;,</span>
            <span class="c1"># update the column to account for the new components of the approximation.</span>
            <span class="n">RJref</span> <span class="o">-=</span> <span class="n">vs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">Jref</span> <span class="p">:</span> <span class="n">Jref</span> <span class="o">+</span> <span class="mi">3</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">us</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>

    <span class="c1"># Return the left and right approximation matrices.</span>
    <span class="c1"># The approximate is such that:</span>
    <span class="c1"># M ~ U_ACA.dot(V_ACA)</span>
    <span class="n">U_ACA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">us</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">V_ACA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">U_ACA</span><span class="p">,</span> <span class="n">V_ACA</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="aca-is-accurate-and-efficient">
<h2><span class="section-number">4.4. </span>ACA+ is accurate and efficient<a class="headerlink" href="#aca-is-accurate-and-efficient" title="Permalink to this headline">¶</a></h2>
<p>And let’s demonstrate the algorithm on the same block we used to demonstrate the SVD compression method. One note here: I set the error tolerance to <code class="docutils literal notranslate"><span class="pre">eps</span> <span class="pre">/</span> <span class="pre">50.0</span></code> because the error tolerance from the ACA algorithm is imprecise. The factor of 50 is large enough to be very confident that the error will be less than or equal to <code class="docutils literal notranslate"><span class="pre">eps</span></code>. We’ll see in the next section how to get a tighter tolerance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U_ACA</span><span class="p">,</span> <span class="n">V_ACA</span> <span class="o">=</span> <span class="n">ACA_plus</span><span class="p">(</span>
    <span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="k">lambda</span> <span class="n">Istart</span><span class="p">,</span> <span class="n">Iend</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="n">Istart</span><span class="p">:</span><span class="n">Iend</span><span class="p">,</span> <span class="p">:],</span>
    <span class="k">lambda</span> <span class="n">Jstart</span><span class="p">,</span> <span class="n">Jend</span><span class="p">:</span> <span class="n">block</span><span class="p">[:,</span> <span class="n">Jstart</span><span class="p">:</span><span class="n">Jend</span><span class="p">],</span>
    <span class="n">eps</span> <span class="o">/</span> <span class="mf">50.0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pivot row= 221, pivot col=2917, step size=5.593e-03, tolerance=2.000e-10
pivot row= 220, pivot col=2918, step size=5.590e-03, tolerance=2.000e-10
pivot row=  29, pivot col=2719, step size=3.061e-03, tolerance=2.000e-10
pivot row=  28, pivot col=2720, step size=3.072e-03, tolerance=2.000e-10
pivot row= 298, pivot col=2996, step size=6.632e-04, tolerance=2.000e-10
pivot row= 118, pivot col=2828, step size=6.273e-04, tolerance=2.000e-10
pivot row= 107, pivot col=2815, step size=7.560e-04, tolerance=2.000e-10
pivot row= 599, pivot col=2995, step size=5.317e-04, tolerance=2.000e-10
pivot row=2702, pivot col=2700, step size=1.364e-04, tolerance=2.000e-10
pivot row=   3, pivot col=   2, step size=1.370e-04, tolerance=2.000e-10
pivot row= 297, pivot col= 299, step size=9.059e-05, tolerance=2.000e-10
pivot row=2701, pivot col= 101, step size=1.407e-04, tolerance=2.000e-10
pivot row=2810, pivot col= 142, step size=1.168e-04, tolerance=2.000e-10
pivot row= 219, pivot col=2960, step size=3.189e-05, tolerance=2.000e-10
pivot row=   2, pivot col=2808, step size=7.423e-05, tolerance=2.000e-10
pivot row= 161, pivot col=2869, step size=2.313e-05, tolerance=2.000e-10
pivot row= 111, pivot col=2774, step size=4.236e-05, tolerance=2.000e-10
pivot row=2999, pivot col=2997, step size=4.235e-05, tolerance=2.000e-10
pivot row= 263, pivot col=2928, step size=1.205e-05, tolerance=2.000e-10
pivot row=  45, pivot col=2702, step size=7.899e-06, tolerance=2.000e-10
pivot row=   1, pivot col= 803, step size=1.101e-05, tolerance=2.000e-10
pivot row=  71, pivot col=2742, step size=2.492e-06, tolerance=2.000e-10
pivot row=2627, pivot col=2868, step size=6.883e-06, tolerance=2.000e-10
pivot row= 159, pivot col=1235, step size=1.700e-06, tolerance=2.000e-10
pivot row=1232, pivot col=1801, step size=1.790e-06, tolerance=2.000e-10
pivot row=2995, pivot col=2864, step size=3.872e-06, tolerance=2.000e-10
pivot row= 166, pivot col=1784, step size=2.127e-06, tolerance=2.000e-10
pivot row=1354, pivot col=1949, step size=7.805e-07, tolerance=2.000e-10
pivot row=1043, pivot col=2455, step size=6.554e-07, tolerance=2.000e-10
pivot row=1499, pivot col=2350, step size=6.247e-07, tolerance=2.000e-10
pivot row= 797, pivot col= 297, step size=4.305e-07, tolerance=2.000e-10
pivot row=2997, pivot col=1502, step size=3.025e-07, tolerance=2.000e-10
pivot row= 256, pivot col=2099, step size=1.669e-07, tolerance=2.000e-10
pivot row=1202, pivot col=   1, step size=1.220e-07, tolerance=2.000e-10
pivot row= 980, pivot col=2773, step size=1.585e-07, tolerance=2.000e-10
pivot row= 287, pivot col=2958, step size=1.192e-07, tolerance=2.000e-10
pivot row=1151, pivot col=2965, step size=5.752e-08, tolerance=2.000e-10
pivot row=2700, pivot col=2315, step size=1.643e-07, tolerance=2.000e-10
pivot row=  70, pivot col=1877, step size=1.287e-07, tolerance=2.000e-10
pivot row= 261, pivot col=2888, step size=1.342e-07, tolerance=2.000e-10
pivot row= 137, pivot col= 298, step size=6.915e-08, tolerance=2.000e-10
pivot row=  75, pivot col=2744, step size=3.963e-08, tolerance=2.000e-10
pivot row=2972, pivot col=2772, step size=2.674e-08, tolerance=2.000e-10
pivot row=1174, pivot col=2804, step size=1.857e-08, tolerance=2.000e-10
pivot row=1228, pivot col= 269, step size=1.423e-08, tolerance=2.000e-10
pivot row=2735, pivot col=   0, step size=9.794e-09, tolerance=2.000e-10
pivot row=  11, pivot col=2236, step size=1.040e-08, tolerance=2.000e-10
pivot row= 189, pivot col=1199, step size=8.859e-09, tolerance=2.000e-10
pivot row=2802, pivot col=  99, step size=6.024e-09, tolerance=2.000e-10
pivot row= 190, pivot col=1442, step size=7.604e-09, tolerance=2.000e-10
pivot row= 814, pivot col=2126, step size=4.106e-09, tolerance=2.000e-10
pivot row= 299, pivot col=2701, step size=6.409e-09, tolerance=2.000e-10
pivot row= 191, pivot col=2838, step size=4.836e-09, tolerance=2.000e-10
pivot row=2177, pivot col=2098, step size=4.466e-09, tolerance=2.000e-10
pivot row=1613, pivot col=2904, step size=2.597e-09, tolerance=2.000e-10
pivot row=1973, pivot col=2718, step size=1.892e-09, tolerance=2.000e-10
pivot row=2911, pivot col=  29, step size=3.342e-09, tolerance=2.000e-10
pivot row=2730, pivot col= 249, step size=5.779e-09, tolerance=2.000e-10
pivot row=2934, pivot col= 167, step size=3.386e-09, tolerance=2.000e-10
pivot row= 314, pivot col=1009, step size=1.504e-09, tolerance=2.000e-10
pivot row=  46, pivot col=2978, step size=2.503e-09, tolerance=2.000e-10
pivot row= 280, pivot col=2213, step size=2.066e-09, tolerance=2.000e-10
pivot row=  53, pivot col=2743, step size=4.349e-10, tolerance=2.000e-10
pivot row=2785, pivot col=  39, step size=1.016e-09, tolerance=2.000e-10
pivot row= 403, pivot col=1019, step size=1.478e-09, tolerance=2.000e-10
pivot row=1181, pivot col=2839, step size=4.810e-10, tolerance=2.000e-10
pivot row= 245, pivot col=2982, step size=5.693e-10, tolerance=2.000e-10
pivot row= 647, pivot col=2097, step size=6.857e-10, tolerance=2.000e-10
pivot row=2102, pivot col= 817, step size=2.573e-10, tolerance=2.000e-10
pivot row=  21, pivot col=2708, step size=2.621e-10, tolerance=2.000e-10
pivot row=1121, pivot col=2893, step size=1.041e-10, tolerance=2.000e-10
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U_ACA</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3000, 71)
</pre></div>
</div>
</div>
</div>
<p>Clearly, the process is converging. We’ve produced an accurate approximation with a rank of 72. You can see the step sizes are converging at almost the rate we would expect from the graph of singular value magnitudes above. As we’ll see below, the ACA+ algorithm is achieving similar levels of accuracy to the SVD with only about 50% more rows and columns.</p>
<p>First, let’s check the accuracy of this matrix approximation by looking at the error from a matrix-vector product.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_aca</span> <span class="o">=</span> <span class="n">U_ACA</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V_ACA</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">abs_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_aca</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">l2_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_diff</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">l2_true</span>
<span class="n">l1_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">l1_true</span>
<span class="n">linf_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">linf_true</span>
<span class="n">frob_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">U_ACA</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V_ACA</span><span class="p">)</span> <span class="o">-</span> <span class="n">block</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L1(UVx-y)        = </span><span class="si">{</span><span class="n">l1_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L2(UVx-y)        = </span><span class="si">{</span><span class="n">l2_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Linf(UVx-y)      = </span><span class="si">{</span><span class="n">linf_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;frob error(M-UV) = </span><span class="si">{</span><span class="n">frob_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">err_df</span><span class="p">[</span><span class="s2">&quot;ACA&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">U_ACA</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">l2_err</span><span class="p">,</span> <span class="n">l1_err</span><span class="p">,</span> <span class="n">linf_err</span><span class="p">,</span> <span class="n">frob_err</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L1(UVx-y)        = 8.219936808032213e-09
L2(UVx-y)        = 1.3115374322498997e-08
Linf(UVx-y)      = 2.3520182731878858e-08
frob error(M-UV) = 4.3695129771496895e-10
</pre></div>
</div>
</div>
</div>
<p>Looking great! We specified the tolerance as <code class="docutils literal notranslate"><span class="pre">eps</span></code> for both ACA+ and for the SVD. ACA overshot a little and is giving accuracy substantially better than the SVD. This is because of the lower tolerance we passed. Due to the inherent randomness in the algorithm, sometimes ACA+ won’t overshoot quite as much. Either way, for a confident tolerance with ACA+ we need to overshoot a little bit because ACA+ is using only local information from the current iteration to decide when to stop whereas the SVD is able to be absolutely certain about the singular values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">l1_mvp_errs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">frob_errs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">ACA_plus</span><span class="p">(</span>
        <span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="k">lambda</span> <span class="n">Istart</span><span class="p">,</span> <span class="n">Iend</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="n">Istart</span><span class="p">:</span><span class="n">Iend</span><span class="p">,</span> <span class="p">:],</span>
        <span class="k">lambda</span> <span class="n">Jstart</span><span class="p">,</span> <span class="n">Jend</span><span class="p">:</span> <span class="n">block</span><span class="p">[:,</span> <span class="n">Jstart</span><span class="p">:</span><span class="n">Jend</span><span class="p">],</span>
        <span class="n">eps</span> <span class="o">/</span> <span class="mf">50.0</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">terms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">y_aca</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">l1_mvp_errs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_aca</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">))</span> <span class="o">/</span> <span class="n">l1_true</span><span class="p">)</span>
    <span class="n">frob_errs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">U</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="o">-</span> <span class="n">block</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>Before moving on, I wanted to demonstrate that the randomness inherent in the reference row/column selection in the ACA+ algorithm is not affecting the end result. See below for histograms showing the distribution of the number of approximation terms and the <span class="math notranslate nohighlight">\(L^1\)</span> matrix-vector error of the approximate matrix. As you can see, ACA+ is producing consistent results within a small range of rank. The matrix-vector error is also consistently and acceptably good. The Frobenius error is also 5-50x below the tolerance specified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Approximation rank&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">terms</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">85</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;rank of approximation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;$L^1$ matrix-vector error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">l1_mvp_errs</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\log_</span><span class="si">{10}</span><span class="s2">(\|E\|^1)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Frobenius Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">frob_errs</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">eps</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\|E\|_</span><span class="si">{F}</span><span class="s2">)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/low_rank_47_0.png" src="../_images/low_rank_47_0.png" />
</div>
</div>
</div>
<div class="section" id="svd-recompression">
<h2><span class="section-number">4.5. </span>SVD Recompression<a class="headerlink" href="#svd-recompression" title="Permalink to this headline">¶</a></h2>
<p>In some sense, the approximation we built with ACA is too good. Most of the time, we’re getting Frobenius errors much lower than we asked for. But there’s some inconsistency in the output error. This inconsistency is why we asked for <code class="docutils literal notranslate"><span class="pre">eps/50.0</span></code> as the convergence criterion. However, we would ideally want an algorithm that <em>juuust barely</em> meets the tolerance condition. Unfortunately that’s quite difficult when the algorithm is stuck with limited information about the matrix under consideration. So, in this final section I’ll demonstrate a simple post-processing method that will reduce the size/cost of the approximation to more precisely fit the requested tolerance.</p>
<p>The basic idea is quite simple: just do another compression of the ACA approximation matrices with the singular value decomposition. We wanted to avoid doing an SVD on the original full-size matrix block, but now that we have constructed a reduced expression, it can be worthwhile to do the extra effort to reduce it even further with an exact SVD.</p>
<p>How do we do that? I’ve already built an approximation that looks like:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7f2b1bb7-7931-4902-b27c-b78baea7a7fc">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-7f2b1bb7-7931-4902-b27c-b78baea7a7fc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
M \approx UV^T
\end{equation}\]</div>
<p>Basically <span class="math notranslate nohighlight">\(V^T\)</span> maps from a high-dimensional space to a low-dimensional space and then <span class="math notranslate nohighlight">\(U\)</span> maps back from the low-dimensional space to the high-dimensional space. So, let’s first extract the low-dimensional operation. To do that, perform a QR decomposition on both <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>. Then we have:</p>
<div class="amsmath math notranslate nohighlight" id="equation-90a5ffa1-3eea-4cf0-9d2a-63f07b1dee31">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-90a5ffa1-3eea-4cf0-9d2a-63f07b1dee31" title="Permalink to this equation">¶</a></span>\[\begin{equation}
UV^T = Q_UR_UR_V^TQ_V^T
\end{equation}\]</div>
<p>One way of thinking about this is that <span class="math notranslate nohighlight">\(Q_V^T\)</span> and <span class="math notranslate nohighlight">\(Q_U\)</span> are just rotations now and <span class="math notranslate nohighlight">\(R_UR_V^T\)</span> is a square matrix in the low-dimensional subspace that is performing the non-rotational action of <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>Now, take the SVD of that small action matrix:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f45bbde2-2f5c-42ea-9102-fb02e3f4eeac">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-f45bbde2-2f5c-42ea-9102-fb02e3f4eeac" title="Permalink to this equation">¶</a></span>\[\begin{equation}
R_UR_V^T = W \Sigma Z^T
\end{equation}\]</div>
<p>And we’ll filter on the singular values in <span class="math notranslate nohighlight">\(\Sigma\)</span> to reduce the rank of the matrix just like we did before with the full SVD. That’s it! To reconstruct the approximation, we just need:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f136c724-5eb5-40de-b9d7-101f0db269c5">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-f136c724-5eb5-40de-b9d7-101f0db269c5" title="Permalink to this equation">¶</a></span>\[\begin{align}
\overline{U} &amp;= Q_UW\Sigma \\
\overline{V} &amp;= ZQ_V \\
M &amp;\approx \overline{U}\overline{V}^T
\end{align}\]</div>
<p>The transformation into code is super straightforward!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QU</span><span class="p">,</span> <span class="n">RU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">U_ACA</span><span class="p">)</span>
<span class="n">QV</span><span class="p">,</span> <span class="n">RV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">V_ACA</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">W</span><span class="p">,</span> <span class="n">SIG</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">RU</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">RV</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reverse the list of singular values and sum them to compute the</span>
<span class="c1"># error from each level of truncation.</span>
<span class="n">frob_K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">SIG</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">frob_K</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span>
<span class="n">r</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recompressing from rank </span><span class="si">{</span><span class="n">SIG</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> to rank </span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Recompressing from rank 71 to rank 40
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U_ACA2</span> <span class="o">=</span> <span class="n">QU</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">]</span> <span class="o">*</span> <span class="n">SIG</span><span class="p">[:</span><span class="n">r</span><span class="p">])</span>
<span class="n">V_ACA2</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">QV</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just seven lines of code to recompress by almost a factor of 2! This will make later matrix-vector products almost twice as fast.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_aca_2</span> <span class="o">=</span> <span class="n">U_ACA2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V_ACA2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">abs_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_aca_2</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">l2_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_diff</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">l2_true</span>
<span class="n">l1_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">l1_true</span>
<span class="n">linf_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">linf_true</span>
<span class="n">frob_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">U_ACA2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V_ACA2</span><span class="p">)</span> <span class="o">-</span> <span class="n">block</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L1(UVx-y)        = </span><span class="si">{</span><span class="n">l1_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L2(UVx-y)        = </span><span class="si">{</span><span class="n">l2_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Linf(UVx-y)      = </span><span class="si">{</span><span class="n">linf_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;frob error(M-UV) = </span><span class="si">{</span><span class="n">frob_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">err_df</span><span class="p">[</span><span class="s2">&quot;Recompress&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">U_ACA2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">l2_err</span><span class="p">,</span> <span class="n">l1_err</span><span class="p">,</span> <span class="n">linf_err</span><span class="p">,</span> <span class="n">frob_err</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L1(UVx-y)        = 1.5730624490743533e-08
L2(UVx-y)        = 2.058148695561588e-08
Linf(UVx-y)      = 5.0074610366361435e-08
frob error(M-UV) = 8.910902539999634e-09
</pre></div>
</div>
</div>
</div>
<p>As you can see in the table below, the amazing thing here is that ACA and then SVD recompression can recover a matrix approximation that is almost exactly as good as the compression that is computed using a direct SVD. The error in the table between the “SVD” and “Recompress” is shockingly close.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">err_df</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Rank</th>
      <th>L2(UVx-y)</th>
      <th>L1(UVx-y)</th>
      <th>Linf(UVx-y)</th>
      <th>Frobenius(M-UV)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>true</th>
      <td>3000.0</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
    </tr>
    <tr>
      <th>SVD</th>
      <td>40.0</td>
      <td>1.572950e-08</td>
      <td>1.153585e-08</td>
      <td>4.840349e-08</td>
      <td>8.902174e-09</td>
    </tr>
    <tr>
      <th>ACA</th>
      <td>71.0</td>
      <td>1.311537e-08</td>
      <td>8.219937e-09</td>
      <td>2.352018e-08</td>
      <td>4.369513e-10</td>
    </tr>
    <tr>
      <th>Recompress</th>
      <td>40.0</td>
      <td>2.058149e-08</td>
      <td>1.573062e-08</td>
      <td>5.007461e-08</td>
      <td>8.910903e-09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And to finish this section, I’ll repeat this process many times to demonstrate that the randomness of the ACA+ does not affect the ability of the SVD recompression to produce almost exactly the same approximation  every time!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SVD_recompress</span><span class="p">(</span><span class="n">U_ACA</span><span class="p">,</span> <span class="n">V_ACA</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recompress an ACA matrix approximation via SVD.</span>

<span class="sd">    :param U_ACA: The left-hand approximation matrix.</span>
<span class="sd">    :param V_ACA: The right-hand approximation matrix.</span>
<span class="sd">    :param eps: The tolerance of the approximation. The convergence condition is</span>
<span class="sd">        in terms of the difference in Frobenius norm between the target matrix</span>
<span class="sd">        and the approximation.</span>

<span class="sd">    :return U_SVD: The SVD recompressed left-hand approximation matrix.</span>
<span class="sd">    :return V_SVD: The SVD recompressed right-hand approximation matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">UQ</span><span class="p">,</span> <span class="n">UR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">U_ACA</span><span class="p">)</span>
    <span class="n">VQ</span><span class="p">,</span> <span class="n">VR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">V_ACA</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">SIG</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">UR</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">VR</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>

    <span class="n">frob_K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">SIG</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">frob_K</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span>

    <span class="n">U</span> <span class="o">=</span> <span class="n">UQ</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">]</span> <span class="o">*</span> <span class="n">SIG</span><span class="p">[:</span><span class="n">r</span><span class="p">])</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">VQ</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">l1_mvp_errs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">frob_errs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">ACA_plus</span><span class="p">(</span>
        <span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">block</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="k">lambda</span> <span class="n">Istart</span><span class="p">,</span> <span class="n">Iend</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="n">Istart</span><span class="p">:</span><span class="n">Iend</span><span class="p">,</span> <span class="p">:],</span>
        <span class="k">lambda</span> <span class="n">Jstart</span><span class="p">,</span> <span class="n">Jend</span><span class="p">:</span> <span class="n">block</span><span class="p">[:,</span> <span class="n">Jstart</span><span class="p">:</span><span class="n">Jend</span><span class="p">],</span>
        <span class="n">eps</span> <span class="o">/</span> <span class="mf">50.0</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">SVD_recompress</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">terms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">y_aca</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">l1_mvp_errs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_aca</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">))</span> <span class="o">/</span> <span class="n">l1_true</span><span class="p">)</span>
    <span class="n">frob_errs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">U</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="o">-</span> <span class="n">block</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Approximation rank&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">terms</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;rank of approximation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Frobenius Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">frob_errs</span><span class="p">),</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">eps</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">eps</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">),</span> <span class="mi">11</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">eps</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\|E\|_</span><span class="si">{F}</span><span class="s2">)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/low_rank_60_0.png" src="../_images/low_rank_60_0.png" />
</div>
</div>
<p>I think it’s worth stopping for a moment to appreciate how cool math can be. We started from an algorithm that involves both some randomness and a severe lack of information on the matrix being approximated. And from that algorithm, we’ve built an almost perfectly consistent algorithm for producing a precise approximate matrix. In the SVD recompressed results, there’s exactly zero variation in the rank and almost zero variation in the error. And the error is just barely on the correct side of the tolerance!</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tdes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="free_matvec.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3. </span>Minimizing memory usage: a matrix-free iterative solver</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="hmatrix.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>GPU-accelerated hierarchical matrices for triangular dislocation elements.</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By T. Ben Thompson<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>